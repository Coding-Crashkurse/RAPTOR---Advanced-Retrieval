{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('data', glob=\"**/*.txt\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(docs)\n",
    "texts = [doc.page_content for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [num_tokens_from_string(t) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(counts, bins=30, color=\"blue\", edgecolor=\"black\", alpha=0.7)\n",
    "plt.title(\"Histogram of Token Counts\")\n",
    "plt.xlabel(\"Token Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis=\"y\", alpha=0.75)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "d_reversed = list(reversed(d_sorted))\n",
    "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
    "    [doc.page_content for doc in d_reversed]\n",
    ")\n",
    "print(\n",
    "    \"Num tokens in all context: %s\"\n",
    "    % num_tokens_from_string(concatenated_content)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_embeddings = [embedding_model.embed_query(txt) for txt in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import umap\n",
    "\n",
    "def reduce_cluster_embeddings(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    n_neighbors: Optional[int] = None,\n",
    "    metric: str = \"cosine\",\n",
    ") -> np.ndarray:\n",
    "    if n_neighbors is None:\n",
    "        n_neighbors = int((len(embeddings) - 1) ** 0.5)\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=n_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "dim = 2\n",
    "global_embeddings_reduced = reduce_cluster_embeddings(global_embeddings, dim)\n",
    "global_embeddings_reduced[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(global_embeddings_reduced[:, 0], global_embeddings_reduced[:, 1], alpha=0.5)\n",
    "plt.title(\"Global Embeddings\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def get_optimal_clusters(embeddings: np.ndarray, max_clusters: int = 50, random_state: int = 1234):\n",
    "    max_clusters = min(max_clusters, len(embeddings))\n",
    "    bics = [GaussianMixture(n_components=n, random_state=random_state).fit(embeddings).bic(embeddings)\n",
    "            for n in range(1, max_clusters)]\n",
    "    return np.argmin(bics) + 1\n",
    "\n",
    "def gmm_clustering(embeddings: np.ndarray, threshold: float, random_state: int = 0):\n",
    "    n_clusters = get_optimal_clusters(embeddings)\n",
    "    gm = GaussianMixture(n_components=n_clusters, random_state=random_state).fit(embeddings)\n",
    "    probs = gm.predict_proba(embeddings)\n",
    "    labels = [np.where(prob > threshold)[0] for prob in probs]\n",
    "    return labels, n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, _ = gmm_clustering(global_embeddings_reduced, threshold=0.5)\n",
    "\n",
    "plot_labels = np.array([label[0] if len(label) > 0 else -1 for label in labels])\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "unique_labels = np.unique(plot_labels)\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_labels)))\n",
    "\n",
    "for label, color in zip(unique_labels, colors):\n",
    "    mask = plot_labels == label\n",
    "    plt.scatter(global_embeddings_reduced[mask, 0], global_embeddings_reduced[mask, 1], color=color, label=f'Cluster {label}', alpha=0.5)\n",
    "\n",
    "plt.title(\"Cluster Visualization of Global Embeddings\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "simple_labels = [label[0] if len(label) > 0 else -1 for label in labels]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Text': texts,\n",
    "    'Embedding': list(global_embeddings_reduced),\n",
    "    'Cluster': simple_labels\n",
    "})\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_cluster_texts(df):\n",
    "    clustered_texts = {}\n",
    "    for cluster in df['Cluster'].unique():\n",
    "        cluster_texts = df[df['Cluster'] == cluster]['Text'].tolist()\n",
    "        clustered_texts[cluster] = \" --- \".join(cluster_texts)\n",
    "    return clustered_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_texts = format_cluster_texts(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = \"\"\"You are an assistant to create a detailed summary of the text input prodived.\n",
    "Text:\n",
    "{text}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "chain = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = {}\n",
    "for cluster, text in clustered_texts.items():\n",
    "    summary = chain.invoke({\"text\": text})\n",
    "    summaries[cluster] = summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_summaries = [embedding_model.embed_query(summary) for summary in summaries.values()]\n",
    "\n",
    "embedded_summaries_np = np.array(embedded_summaries)\n",
    "\n",
    "labels, _ = gmm_clustering(embedded_summaries_np, threshold=0.5)\n",
    "\n",
    "simple_labels = [label[0] if len(label) > 0 else -1 for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_summaries = {}\n",
    "for i, label in enumerate(simple_labels):\n",
    "    if label not in clustered_summaries:\n",
    "        clustered_summaries[label] = []\n",
    "    clustered_summaries[label].append(list(summaries.values())[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summaries = {}\n",
    "for cluster, texts in clustered_summaries.items():\n",
    "    combined_text = ' '.join(texts)\n",
    "    summary = chain.invoke({\"text\": combined_text})\n",
    "    final_summaries[cluster] = summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: \"The text provides a detailed summary of a culinary journey through various regions of Italy, highlighting the unique flavors and traditions of each area. It explores the rich culinary heritage of Tuscany, Sicily, Venice, Rome, and Naples, showcasing traditional dishes and ingredients that have been cherished for generations. Italian cuisine is celebrated for its simplicity, freshness, and flavor, with an emphasis on natural ingredients and the Mediterranean diet.\\n\\nThe text emphasizes the importance of supporting local farmers and producers to ensure the integrity of ingredients and contribute to a vibrant food culture. It invites readers to embark on a gastronomic journey through Italy, where each dish is a celebration of tradition, flavor, and the soulful spirit of Italian cuisine. The text also provides a detailed description of various Italian dishes from different regions, highlighting the flavors, ingredients, and cultural significance of each dish.\\n\\nBella Vista is described as a culinary sanctuary that offers a journey through the vibrant flavors and time-honored traditions of Italian cuisine. The founder, Giovanni, has a deep passion for sharing the soulful flavors of his homeland and has created a warm and inviting atmosphere that reflects his commitment to excellence and reverence for culinary traditions. The restaurant sources the finest seasonal ingredients from local farmers, fishermen, and artisans to create dishes that are a love letter to Italian culture.\\n\\nThe text also provides a detailed overview of the different chapters in a book or story that focuses on the culinary and cultural aspects of various regions in Italy. It covers topics such as the flavors of Sicily, Venetian elegance, Roman revival, Neapolitan soul, and the roots of inspiration. The journey begins with the birth of Bella Vista, a community of food lovers who value the art of hospitality and culinary vision.\\n\\nGiovanni Di Napoli's culinary journey is also detailed, starting from his upbringing in Naples and his immersion in the rich tapestry of Neapolitan culture and culinary traditions. Guided by his Nonna Rosa, Giovanni developed a deep appreciation for fresh, locally sourced ingredients and time-honored cooking techniques. Today, Giovanni is dedicated to creating an atmosphere of warmth, comfort, and joy in his restaurant, continuously innovating his menu with seasonal specials and reinterpretations of classic dishes. He believes in the power of food to bring people together, bridge divides, and create lasting memories, passing down traditions, recipes, and a deep-seated belief in the nourishing and uplifting power of food to his own family.\"}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_from_df = df['Text'].tolist()\n",
    "texts_from_clustered_texts = list(clustered_texts.values())\n",
    "texts_from_final_summaries = list(final_summaries.values())\n",
    "\n",
    "combined_texts = texts_from_df + texts_from_clustered_texts + texts_from_final_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Now, use all_texts to build the vectorstore with Chroma\n",
    "vectorstore = Chroma.from_texts(texts=combined_texts, embedding=embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or buffer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m final_number\n\u001b[1;32m---> 11\u001b[0m final_number \u001b[38;5;241m=\u001b[39m \u001b[43madjust_final_number\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWho is the owner of the restaurant\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[53], line 5\u001b[0m, in \u001b[0;36madjust_final_number\u001b[1;34m(string, max_threshold, initial_number)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m final_number \u001b[38;5;241m<\u001b[39m max_threshold:\n\u001b[0;32m      4\u001b[0m     retriever \u001b[38;5;241m=\u001b[39m vectorstore\u001b[38;5;241m.\u001b[39mas_retriever(search_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m\"\u001b[39m: final_number})\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnum_tokens_from_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m max_threshold:\n\u001b[0;32m      6\u001b[0m         final_number \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[41], line 6\u001b[0m, in \u001b[0;36mnum_tokens_from_string\u001b[1;34m(string)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of tokens in a text string.\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m encoding \u001b[38;5;241m=\u001b[39m tiktoken\u001b[38;5;241m.\u001b[39mget_encoding(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcl100k_base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m num_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m num_tokens\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\raptor\\app\\Lib\\site-packages\\tiktoken\\core.py:116\u001b[0m, in \u001b[0;36mEncoding.encode\u001b[1;34m(self, text, allowed_special, disallowed_special)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(disallowed_special, \u001b[38;5;28mfrozenset\u001b[39m):\n\u001b[0;32m    115\u001b[0m         disallowed_special \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m(disallowed_special)\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m match \u001b[38;5;241m:=\u001b[39m \u001b[43m_special_token_regex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisallowed_special\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    117\u001b[0m         raise_disallowed_special_token(match\u001b[38;5;241m.\u001b[39mgroup())\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# https://github.com/PyO3/pyo3/pull/3632\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or buffer"
     ]
    }
   ],
   "source": [
    "def adjust_final_number(string: str, max_threshold: int, initial_number: int) -> int:\n",
    "    final_number = initial_number\n",
    "    while final_number < max_threshold:\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": final_number})\n",
    "        docs = retriever.get_relevant_documents(string)\n",
    "        text = \"\".join([doc.page_content for doc in docs])\n",
    "        if num_tokens_from_string(text) < max_threshold:\n",
    "            final_number += 1\n",
    "        else:\n",
    "            break\n",
    "    return final_number\n",
    "\n",
    "final_number = adjust_final_number(\"Who is the owner of the restaurant\", 10000, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.as_retriever(search_kwargs={\"k\": final_number})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Answer the following question:\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question\n",
    "rag_chain.invoke(\"Who is the owner of the restaurant\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
